{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f895b1-0ea9-4e77-9e97-f9693530aafd",
   "metadata": {},
   "source": [
    "60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
    "\n",
    "Here are the classes in the dataset, as well as 10 random images from each:\n",
    "airplane \t\t\t\t\t\t\t\t\t\t\n",
    "automobile \t\t\t\t\t\t\t\t\t\t\n",
    "bird \t\t\t\t\t\t\t\t\t\t\n",
    "cat \t\t\t\t\t\t\t\t\t\t\n",
    "deer \t\t\t\t\t\t\t\t\t\t\n",
    "dog \t\t\t\t\t\t\t\t\t\t\n",
    "frog \t\t\t\t\t\t\t\t\t\t\n",
    "horse \t\t\t\t\t\t\t\t\t\t\n",
    "ship \t\t\t\t\t\t\t\t\t\t\n",
    "truck \t\t\t\t\t\t\t\t\t\t\n",
    "\n",
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0779ae9b-9a1f-4efb-9f52-bc8e65eb1a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "343c7fbe-8e54-4d22-ade0-65d62ec2ed1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './data/cifar-10-batches-py/'\n",
    "file = 'batches.meta'\n",
    "dict_names = unpickle(data_path + file)\n",
    "class_names = [names.decode('utf-8') for names in dict_names[b'label_names']]\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ed7150e3-9f2f-49cf-aa29-6e992c6c42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3abbfedc-a7e8-4e66-8298-9c760b769e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cytech/.local/lib/python3.8/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = './data'\n",
    "processing = v2.Compose([\n",
    "    v2.ToTensor(),\n",
    "])\n",
    "cifar10_train = datasets.CIFAR10(data_path, train=True, transform=processing, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1bbedf4b-4d84-469b-b93b-8134d5a7f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(cifar10_train, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(cifar10_val, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "31bd2be2-79c5-4d9b-806b-9f596baaf1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 51200000])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.stack([img_t for img_t, _ in cifar10_train], dim=3)\n",
    "t.view(3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "125f8ca6-dede-47d9-b7b6-17b9919e2af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4914, 0.4822, 0.4465]), tensor([0.2470, 0.2435, 0.2616]))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.view(3, -1)\n",
    "t.mean(dim=1), t.std(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "de66eebc-16d1-4f3c-bbf3-49699be6c55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar10_train[0]\n",
    "img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
