{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9ea1b4-ebed-49b6-92ed-42e80b4a495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from data import getDataLoaders\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb2fe4e0-aa7d-45bc-abf0-da998a3c4f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ResNet-18 model from pytorch and display its architecture \n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "resnet18 = models.resnet18(weights='DEFAULT').to(device)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017b5d5-b2a5-4a50-8baa-bb44916b449e",
   "metadata": {},
   "source": [
    "In this pre-trained ResNet model, two key adjustments need to be made: the first and last layers. ResNet was originally trained on the ImageNet dataset, which consists of images that are 224x224 pixels and classified into a thousand categories. Here are the two main issues and their solutions:\n",
    "\n",
    "1) **Input image size and normalization**: Pre-trained models expect input images to be normalized in a specific way, with mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are at least 224 pixels (cf [PyTorch ResNet Documentation](https://pytorch.org/hub/pytorch_vision_resnet/)). Moreover, the model expects the input images to be normalized with the following mean and standard deviation values (calculated from ImageNet data):\n",
    "    - `mean = [0.485, 0.456, 0.406]`\n",
    "    - `std = [0.229, 0.224, 0.225]`\n",
    "   \n",
    "***Solution***: Since CIFAR-10 images are 32x32 pixels, we can either resize them to match the pre-trained model's input size or adapt the first layer to fit the smaller input. Resizing to 224x224 can cause distortion, loss of detail, and significantly increase computation time due to the larger input size. Instead, modifying the first layer with a smaller 3x3 kernel is more efficient, preserving details and reducing computational cost compared to the original 7x7 kernel. We also omit the maxpool layer since its pooling operation is unnecessary for such small images.\n",
    "\n",
    "2) **Output layer**: The final fully connected layer, `(fc): Linear(in_features=512, out_features=1000, bias=True)`, is designed to output 1,000 features, corresponding to the 1,000 classes of ImageNet. \n",
    "\n",
    "***Solution***: The CIFAR-10 dataset has 10 different classes. Therefore, we need to adjust the `out_features` parameter in the final fully connected layer: `(fc): Linear(in_features=512, out_features=10, bias=True).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7a93a3-047e-46c9-a062-41b2078fadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = models.resnet18(weights=weights)\n",
    "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.model.maxpool = nn.Identity()\n",
    "        self.model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b51887-ee83-4c34-b610-b040059b033a",
   "metadata": {},
   "source": [
    "Initially, we will test the model with default weights. Consequently, we should use the provided normalization values since the model was originally trained on a different dataset, and these values help ensure the input is consistent with what the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f437a28d-09a1-4f3f-9bf8-aba7db076ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data/cifar-10-batches-py/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "                 ToImage()\n",
       "                 ToDtype(scale=True)\n",
       "                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       "           )"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_net = ResNet(weights='DEFAULT')\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_loader, val_loader = getDataLoaders(mean=mean, std=std)\n",
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476e52cb-63fc-4b81-935b-143a4e5ffeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(model, train_loader, val_loader, device):\n",
    "    model = model.to(device=device)\n",
    "    model.eval()\n",
    "    for name, loader in zip((['train', 'val']), ([train_loader, val_loader])):\n",
    "        correct = 0\n",
    "        count = torch.zeros(10).to(device)\n",
    "        total = len(loader.dataset)\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "           \n",
    "            with torch.inference_mode():\n",
    "                outputs = model(imgs)\n",
    "                pred = torch.argmax(outputs, dim=1)\n",
    "            correct += int((pred == labels).sum())\n",
    "            count += torch.bincount(pred, minlength=10)\n",
    "\n",
    "        print(f\"Score {name}: {correct} / {total}\",\n",
    "              f\"\\nAccuracy {name}: {(correct / total)*100:.2f}%\",\n",
    "              f\"\\nDistribution {name} (in %): [{', '.join([f'{(c / total * 100):.2f}' for c in count])}]\")\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3497857-6b42-471b-a19b-e5b6907ae33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 4973 / 50000 \n",
      "Accuracy train: 9.95% \n",
      "Distribution train (in %): [0.00, 0.00, 2.74, 0.00, 0.44, 0.03, 85.88, 0.34, 0.00, 10.57]\n",
      "\n",
      "Score val: 1015 / 10000 \n",
      "Accuracy val: 10.15% \n",
      "Distribution val (in %): [0.00, 0.00, 2.73, 0.00, 0.46, 0.01, 85.37, 0.62, 0.00, 10.81]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_accuracy(\n",
    "    model=default_net,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0259b4-7c36-47f7-a4a0-c42d3baa2986",
   "metadata": {},
   "source": [
    "As expected, the model needs to be trained properly to produce meaningful results on the CIFAR-10 dataset. Applying basic transformations alone isn’t enough. The current accuracy of 10% suggests that the model is effectively making random guesses, as we'd expect from choosing a class purely by chance.\n",
    "\n",
    "More interestingly, the class distribution reveals a clear bias. The model is heavily favoring certain classes while barely predicting others.\n",
    "\n",
    "Now, let's train the model on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60604cac-586d-4c74-b834-1db3ab374e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data/cifar-10-batches-py/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "                 ToImage()\n",
       "                 ToDtype(scale=True)\n",
       "                 Normalize(mean=[tensor(0.4914), tensor(0.4822), tensor(0.4465)], std=[tensor(0.2470), tensor(0.2435), tensor(0.2616)], inplace=False)\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CIFAR-10 normalization\n",
    "train_loader, val_loader = getDataLoaders()\n",
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "02be7486-3888-47e4-90c7-f8270a0f89d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-17 20:13:17.859327, Epoch: 1, Train Loss: 1.3734, Accuracy: 50.46%\n",
      "2024-09-17 20:14:14.363153, Epoch: 2, Train Loss: 0.8163, Accuracy: 71.27%\n",
      "2024-09-17 20:15:11.154941, Epoch: 3, Train Loss: 0.5688, Accuracy: 79.89%\n",
      "2024-09-17 20:16:07.971020, Epoch: 4, Train Loss: 0.3904, Accuracy: 86.29%\n",
      "2024-09-17 20:17:04.778036, Epoch: 5, Train Loss: 0.2539, Accuracy: 91.13%\n",
      "2024-09-17 20:18:01.656273, Epoch: 6, Train Loss: 0.1548, Accuracy: 94.46%\n",
      "2024-09-17 20:18:58.527789, Epoch: 7, Train Loss: 0.1277, Accuracy: 95.55%\n",
      "2024-09-17 20:19:55.473683, Epoch: 8, Train Loss: 0.0717, Accuracy: 97.47%\n",
      "2024-09-17 20:20:52.591468, Epoch: 9, Train Loss: 0.0551, Accuracy: 98.19%\n",
      "2024-09-17 20:21:49.774665, Epoch: 10, Train Loss: 0.0504, Accuracy: 98.27%\n"
     ]
    }
   ],
   "source": [
    "cifar_net = ResNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cifar_net.parameters(), lr=0.01, momentum=0.9)\n",
    "training_loop(\n",
    "    n_epochs=10,\n",
    "    loader=train_loader,\n",
    "    model=cifar_net, \n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed719c8c-1f85-41a0-bb90-4a2cc4b931e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 49083 / 50000 \n",
      "Accuracy train: 98.17% \n",
      "Distribution train (in %): [10.24, 9.82, 10.09, 10.58, 10.02, 9.19, 10.05, 9.88, 9.97, 10.16]\n",
      "\n",
      "Score val: 7852 / 10000 \n",
      "Accuracy val: 78.52% \n",
      "Distribution val (in %): [11.36, 9.25, 10.14, 13.28, 9.68, 6.56, 10.06, 9.02, 9.88, 10.77]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_accuracy(\n",
    "    model=cifar_net,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "afb3af37-fc9c-41ed-b211-0a79e3dc6ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(cifar_net.state_dict(), './models/resnet18.pth')\n",
    "model = ResNet()\n",
    "model.load_state_dict(torch.load('./models/resnet18.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff482c7-6f6b-4e0f-86d2-8011c89f02bf",
   "metadata": {},
   "source": [
    "The results are good considering only 10 epochs of training and minimal modifications! The distribution is as expected, with approximately 10% for each class. However, there is noticeable overfitting: the model's accuracy on the training set reaches about 98%, while its accuracy on the validation set drops below 80%.\n",
    "\n",
    "From now on, we will test multiple models to prevent overfitting. To clearly visualize the performance of each model, we will use TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271c051-3d99-4f89-b971-3f0738a7c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "# torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, device=None, dtype=None)\n",
    "# torch.nn.ReLU(inplace=False)\n",
    "# torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48453511-bb46-44e4-b526-b21bd90b54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super().__init__()\n",
    "        stride = 2 if downsample else 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=out_channels)\n",
    "\n",
    "        if downsample:\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                                  ('conv3', nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=2, bias=False)),\n",
    "                                  ('bn3', nn.BatchNorm2d(num_features=out_channels)),\n",
    "                                ]))\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        #  If downsampling is applied, we need to adjust the residual's dimensions\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "        out += residual\n",
    "\n",
    "        return self.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fd5ca0-f2fa-441d-ae9d-97d68d48dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self, layers, out_channels=64, dropout=0):\n",
    "        super().__init__()\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layers = nn.ModuleDict()\n",
    "\n",
    "        # Creation of the layers \n",
    "        for num_layer in range(1, layers+1):\n",
    "            # No downsampling in the first layer\n",
    "            downsample = True if num_layer > 1 else False\n",
    "\n",
    "            layer_name = f'layer{num_layer}'\n",
    "            layer = self.make_layer(downsample)\n",
    "            self.layers[layer_name] = layer\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.fc = nn.Linear(in_features=self.in_channels, out_features=10)\n",
    "        self.dropout = nn.Dropout(p=dropout) if dropout else None\n",
    "        \n",
    "    def make_layer(self, downsample):\n",
    "        if downsample:\n",
    "            layer = nn.Sequential(OrderedDict([\n",
    "                ('block1', BasicBlock(in_channels=self.in_channels, out_channels=self.in_channels*2, downsample=True)),\n",
    "                ('block2', BasicBlock(in_channels=self.in_channels*2, out_channels=self.in_channels*2))\n",
    "            ]))\n",
    "            # Update in_channels after downsampling\n",
    "            self.in_channels *= 2\n",
    "        else:\n",
    "            layer = nn.Sequential(OrderedDict([\n",
    "                ('block1', BasicBlock(in_channels=self.in_channels, out_channels=self.in_channels)),\n",
    "                ('block2', BasicBlock(in_channels=self.in_channels, out_channels=self.in_channels))\n",
    "            ]))\n",
    "\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        for _, layer in self.layers.items():\n",
    "            out = layer(out)\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        if self.dropout:\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "142de4cc-0fc7-4d2b-b81b-e89556847c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_epochs, model, train_loader, val_loader, loss_fn, optimizer, device, log_dir=\"./runs\"):\n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    total_train = len(train_loader.dataset)\n",
    "    total_val = len(val_loader.dataset)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        # Loss/train metric\n",
    "        loss_train = 0.0\n",
    "        # Acc/train metric\n",
    "        correct_train = 0\n",
    "\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            batch_size = imgs.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item() * batch_size\n",
    "            predicted = torch.argmax(logits, dim=1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            \n",
    "        total_loss_train = loss_train / total_train\n",
    "        total_acc_train = (correct_train / total_train) * 100 \n",
    "\n",
    "        model.eval()\n",
    "        # Loss/val metric\n",
    "        loss_val = 0.0\n",
    "        # Loss/accuracy metric\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                batch_size = imgs.size(0)\n",
    "                \n",
    "                logits = model(imgs)\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "                loss_val += loss.item() * batch_size\n",
    "                predicted = torch.argmax(logits, dim=1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "        total_loss_val = loss_val / total_val\n",
    "        total_acc_val = (correct_val / total_val) * 100\n",
    "\n",
    "        writer.add_scalar('Loss/train', total_loss_train, epoch)\n",
    "        writer.add_scalar('Accuracy/train', total_acc_train, epoch)\n",
    "        writer.add_scalar('Loss/validation', total_loss_val, epoch)\n",
    "        writer.add_scalar('Accuracy/validation', total_acc_val, epoch)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "        print(f'Train Loss: {total_loss_train:.4f}, Train Accuracy: {total_acc_train:.2f}%')\n",
    "        print(f'Validation Loss: {total_loss_val:.4f}, Validation Accuracy: {total_acc_val:.2f}%')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "5c012ad7-f4e7-4cba-a4ca-0d8d260897b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_model in module __main__:\n",
      "\n",
      "train_model(n_epochs, model, train_loader, val_loader, loss_fn, optimizer, device, log_dir='./runs')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df89943a-accf-48f1-a6dd-4bb07399c926",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "train_loader, val_loader = getDataLoaders()\n",
    "weight_decay = 1e-4\n",
    "for dropout in [0.2, 0.3, 0.4]:\n",
    "    model = ResNetLayer(3, dropout=dropout)\n",
    "    model_name = f'3-10-Adam-{dropout}-{weight_decay}'\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=weight_decay)\n",
    "    train_model(\n",
    "        n_epochs=10,\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        log_dir=f'./runs/ResNet18/{model_name}'\n",
    "    )\n",
    "    torch.save(model.state_dict(), f'./models/ResNet18/{model_name}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
