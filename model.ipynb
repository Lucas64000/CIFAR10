{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a9ea1b4-ebed-49b6-92ed-42e80b4a495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from data import getDataLoaders\n",
    "import torch.optim as optim\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb2fe4e0-aa7d-45bc-abf0-da998a3c4f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ResNet-18 model from pytorch and display its architecture \n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "resnet18 = models.resnet18(weights='DEFAULT').to(device)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017b5d5-b2a5-4a50-8baa-bb44916b449e",
   "metadata": {},
   "source": [
    "In this pre-trained ResNet model, two key adjustments need to be made: the first and last layers. ResNet was originally trained on the ImageNet dataset, which consists of images that are 224x224 pixels and classified into a thousand categories. Here are the two main issues and their solutions:\n",
    "\n",
    "1) **Input image size and normalization**: Pre-trained models expect input images to be normalized in a specific way, with mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are at least 224 pixels (cf [PyTorch ResNet Documentation](https://pytorch.org/hub/pytorch_vision_resnet/)). Moreover, the model expects the input images to be normalized with the following mean and standard deviation values (calculated from ImageNet data):\n",
    "    - `mean = [0.485, 0.456, 0.406]`\n",
    "    - `std = [0.229, 0.224, 0.225]`\n",
    "   \n",
    "***Solution***: Since CIFAR-10 images are 32x32 pixels, we can either resize them to match the pre-trained model's input size or adapt the first layer to fit the smaller input. Resizing to 224x224 can cause distortion, loss of detail, and significantly increase computation time due to the larger input size. Instead, modifying the first layer with a smaller 3x3 kernel is more efficient, preserving details and reducing computational cost compared to the original 7x7 kernel. We also omit the maxpool layer since its pooling operation is unnecessary for such small images.\n",
    "\n",
    "2) **Output layer**: The final fully connected layer, `(fc): Linear(in_features=512, out_features=1000, bias=True)`, is designed to output 1,000 features, corresponding to the 1,000 classes of ImageNet. \n",
    "\n",
    "***Solution***: The CIFAR-10 dataset has 10 different classes. Therefore, we need to adjust the `out_features` parameter in the final fully connected layer: `(fc): Linear(in_features=512, out_features=10, bias=True).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4c7a93a3-047e-46c9-a062-41b2078fadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = models.resnet18(weights=weights)\n",
    "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.model.maxpool = nn.Identity()\n",
    "        self.model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b51887-ee83-4c34-b610-b040059b033a",
   "metadata": {},
   "source": [
    "Initially, we will test the model with default weights. Consequently, we should use the provided normalization values since the model was originally trained on a different dataset, and these values help ensure the input is consistent with what the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f437a28d-09a1-4f3f-9bf8-aba7db076ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data/cifar-10-batches-py/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "                 ToImage()\n",
       "                 ToDtype(scale=True)\n",
       "                 Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n",
       "           )"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_net = ResNet(weights='DEFAULT')\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_loader, val_loader = getDataLoaders(mean=mean, std=std)\n",
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476e52cb-63fc-4b81-935b-143a4e5ffeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(model, train_loader, val_loader, device):\n",
    "    model = model.to(device=device)\n",
    "    model.eval()\n",
    "    for name, loader in zip((['train', 'val']), ([train_loader, val_loader])):\n",
    "        correct = 0\n",
    "        count = torch.zeros(10).to(device)\n",
    "        total = len(loader.dataset)\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "           \n",
    "            with torch.inference_mode():\n",
    "                outputs = model(imgs)\n",
    "                pred = torch.argmax(outputs, dim=1)\n",
    "            correct += int((pred == labels).sum())\n",
    "            count += torch.bincount(pred, minlength=10)\n",
    "\n",
    "        print(f\"Score {name}: {correct} / {total}\",\n",
    "              f\"\\nAccuracy {name}: {(correct / total)*100:.2f}%\",\n",
    "              f\"\\nDistribution {name} (in %): [{', '.join([f'{(c / total * 100):.2f}' for c in count])}]\")\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3497857-6b42-471b-a19b-e5b6907ae33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 4973 / 50000 \n",
      "Accuracy train: 9.95% \n",
      "Distribution train (in %): [0.00, 0.00, 2.74, 0.00, 0.44, 0.03, 85.88, 0.34, 0.00, 10.57]\n",
      "\n",
      "Score val: 1015 / 10000 \n",
      "Accuracy val: 10.15% \n",
      "Distribution val (in %): [0.00, 0.00, 2.73, 0.00, 0.46, 0.01, 85.37, 0.62, 0.00, 10.81]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_accuracy(\n",
    "    model=default_net,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0259b4-7c36-47f7-a4a0-c42d3baa2986",
   "metadata": {},
   "source": [
    "As expected, the model needs to be trained properly to produce meaningful results on the CIFAR-10 dataset. Applying basic transformations alone isnâ€™t enough. The current accuracy of 10% suggests that the model is effectively making random guesses, as we'd expect from choosing a class purely by chance.\n",
    "\n",
    "More interestingly, the class distribution reveals a clear bias. The model is heavily favoring certain classes while barely predicting others.\n",
    "\n",
    "Now, let's train the model on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adfdde37-4cbc-45cd-a636-fd9e20885252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, loader, model, optimizer, loss_fn, device):\n",
    "    model = model.to(device)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()  \n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / total\n",
    "        accuracy = correct / total * 100\n",
    "        \n",
    "        \n",
    "        print(f\"{datetime.datetime.now()}, Epoch: {epoch}, Train Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60604cac-586d-4c74-b834-1db3ab374e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data/cifar-10-batches-py/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "                 ToImage()\n",
       "                 ToDtype(scale=True)\n",
       "                 Normalize(mean=[tensor(0.4914), tensor(0.4822), tensor(0.4465)], std=[tensor(0.2470), tensor(0.2435), tensor(0.2616)], inplace=False)\n",
       "           )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CIFAR-10 normalization\n",
    "train_loader, val_loader = getDataLoaders()\n",
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "02be7486-3888-47e4-90c7-f8270a0f89d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-17 20:13:17.859327, Epoch: 1, Train Loss: 1.3734, Accuracy: 50.46%\n",
      "2024-09-17 20:14:14.363153, Epoch: 2, Train Loss: 0.8163, Accuracy: 71.27%\n",
      "2024-09-17 20:15:11.154941, Epoch: 3, Train Loss: 0.5688, Accuracy: 79.89%\n",
      "2024-09-17 20:16:07.971020, Epoch: 4, Train Loss: 0.3904, Accuracy: 86.29%\n",
      "2024-09-17 20:17:04.778036, Epoch: 5, Train Loss: 0.2539, Accuracy: 91.13%\n",
      "2024-09-17 20:18:01.656273, Epoch: 6, Train Loss: 0.1548, Accuracy: 94.46%\n",
      "2024-09-17 20:18:58.527789, Epoch: 7, Train Loss: 0.1277, Accuracy: 95.55%\n",
      "2024-09-17 20:19:55.473683, Epoch: 8, Train Loss: 0.0717, Accuracy: 97.47%\n",
      "2024-09-17 20:20:52.591468, Epoch: 9, Train Loss: 0.0551, Accuracy: 98.19%\n",
      "2024-09-17 20:21:49.774665, Epoch: 10, Train Loss: 0.0504, Accuracy: 98.27%\n"
     ]
    }
   ],
   "source": [
    "cifar_net = ResNet()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cifar_net.parameters(), lr=0.01, momentum=0.9)\n",
    "training_loop(\n",
    "    n_epochs=10,\n",
    "    loader=train_loader,\n",
    "    model=cifar_net, \n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed719c8c-1f85-41a0-bb90-4a2cc4b931e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 49083 / 50000 \n",
      "Accuracy train: 98.17% \n",
      "Distribution train (in %): [10.24, 9.82, 10.09, 10.58, 10.02, 9.19, 10.05, 9.88, 9.97, 10.16]\n",
      "\n",
      "Score val: 7852 / 10000 \n",
      "Accuracy val: 78.52% \n",
      "Distribution val (in %): [11.36, 9.25, 10.14, 13.28, 9.68, 6.56, 10.06, 9.02, 9.88, 10.77]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_accuracy(\n",
    "    model=cifar_net,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "afb3af37-fc9c-41ed-b211-0a79e3dc6ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(cifar_net.state_dict(), './models/resnet18.pth')\n",
    "model = ResNet()\n",
    "model.load_state_dict(torch.load('./models/resnet18.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff482c7-6f6b-4e0f-86d2-8011c89f02bf",
   "metadata": {},
   "source": [
    "The results are good considering only 10 epochs of training and minimal modifications! The distribution is as expected, with approximately 10% for each class. However, there is noticeable overfitting: the model's accuracy on the training set reaches about 98%, while its accuracy on the validation set drops below 80%.\n",
    "\n",
    "From now on, we will test multiple models to prevent overfitting. To clearly visualize the performance of each model, we will use TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1ccb0f-e3a1-4641-a206-6f7d2f31b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "277777e3-57f5-4090-be6d-cf3ed28f576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the writer to the training loop \n",
    "def training_loop_tb(n_epochs, loader, model, optimizer, loss_fn, device, logdir=\"runs/tests\"):\n",
    "    model = model.to(device)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()  \n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / total\n",
    "        accuracy = correct / total * 100\n",
    "        \n",
    "        writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/train', accuracy, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d09a3-74e5-4801-b28f-ed89364824ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample not None => BasicBlock[0] (downsample): Sequential(\n",
    "        (0): Conv2d(in_features, out_features, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "        (1): BatchNorm2d(out_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a826ba6-ebbb-4f84-b07b-64490db3278f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48453511-bb46-44e4-b526-b21bd90b54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, stride=1, downsample=None):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f84f32-57b3-455e-9101-b9773d75dd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetDropout(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = models.resnet18(weights=weights)\n",
    "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.model.maxpool = nn.Identity()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.model.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model.conv1(x)\n",
    "        out = self.model.bn1(out)\n",
    "        out = self.model.relu(out)\n",
    "\n",
    "        out = self.model.layer1(out)\n",
    "        out = self.model.layer2(out)\n",
    "        out = self.model.layer3(out)\n",
    "        out = self.model.layer4(out)\n",
    "\n",
    "        out = self.model.avgpool(out)\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return self.model.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90063e2a-b59c-4679-87c2-0bdfb11f77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifdrop_net = ResNetDropout()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cifdrop_net.parameters(), lr=0.01, momentum=0.9)\n",
    "training_loop_tb(\n",
    "    n_epochs=10,\n",
    "    loader=train_loader,\n",
    "    model=cifdrop_net, \n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    device=device\n",
    ")\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88736b7-0b97-4c60-8ef3-9c8b2e0cd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetSkipLayers(nn.Module):\n",
    "    def __init__(self, num_layers):\n",
    "        self.model = models.resnet18(weights=weights)\n",
    "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.model.maxpool = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdd19807-1954-4147-91e6-8a1db189a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score train: 49058 / 50000 \n",
      "Accuracy train: 98.12% \n",
      "Distribution train (in %): [10.09, 10.11, 9.29, 10.10, 10.28, 10.02, 10.04, 10.38, 9.70, 9.99]\n",
      "\n",
      "Score val: 7890 / 10000 \n",
      "Accuracy val: 78.90% \n",
      "Distribution val (in %): [10.54, 10.31, 6.94, 10.47, 11.00, 9.92, 10.35, 11.77, 8.76, 9.94]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_accuracy(\n",
    "    model=cifdrop_net,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253e4d1-8e71-4c22-98af-0c63492a0053",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
